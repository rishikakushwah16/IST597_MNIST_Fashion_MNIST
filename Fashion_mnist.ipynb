{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71kdFp0QgF4K"
      },
      "source": [
        "# IST597:- Multi-Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2yHcl5xgPV1"
      },
      "source": [
        "## Load the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2DPwxLR2gSLC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "np.random.seed(5510)\n",
        "tf.random.set_seed(5510)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV-3kEaggcO8",
        "outputId": "72952f18-1305-4f1b-9f63-20e3d8ed629b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw78jw6pDqSM"
      },
      "source": [
        "#Get number of Gpu's and id's in the system or else you can also use Nvidia-smi in command prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dk_S2TMg_6_"
      },
      "source": [
        "## Generate random data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "40XlFnwho7D8"
      },
      "outputs": [],
      "source": [
        "size_input = 784\n",
        "size_hidden =[128, 64]\n",
        "size_output = 10\n",
        "number_of_train_examples = 60000\n",
        "number_of_test_examples = 10000\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "# load dataset\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpRqHa3Dq3-X",
        "outputId": "0b4bf818-0b40-4274-8ce3-09e619b58ba9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 2, 1, ..., 8, 1, 5], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RhCEG3fcrP-w"
      },
      "outputs": [],
      "source": [
        "y_train = tf.keras.utils.to_categorical(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GL7kLRPl0VLr"
      },
      "outputs": [],
      "source": [
        "y_test = tf.keras.utils.to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMTdXC1SrSgQ",
        "outputId": "57b2da97-e1f2-4a87-e5fa-9250224cd479"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "y_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YtNd_X5M9Iuz"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.reshape(60000, 28*28)\n",
        "X_test = X_test.reshape(10000, 28*28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_hEngtyr1o2",
        "outputId": "fea9f218-27de-444e-f4b0-30d7df1098b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u9thOVXr-E_",
        "outputId": "0810eccd-8379-4da9-ff8c-6d57f4fb0a4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EOm4zNP49mVZ"
      },
      "outputs": [],
      "source": [
        "X_train = X_train / 255\n",
        "X_test = X_test / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6LRQZJoFtJK",
        "outputId": "8ab4a91b-edf2-4f59-f68f-13517cc09a1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzNCoAuXSY7E",
        "outputId": "2303744e-8ece-4b97-d591-b49e29866442"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGn2kjp7UOC9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYJT5X0CUOoc",
        "outputId": "af63b900-1212-43e0-89b3-c148e62daebd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO7-is5ALn-r",
        "outputId": "217203e0-2e66-4aca-8c25-12b5a1bcaf21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47040000"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "X_train.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1egEb3sLoG7",
        "outputId": "5e2fd8a1-32b1-4273-87c7-fae6d1c0fb89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600000"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "y_train.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-unAB1UeUTAc",
        "outputId": "2968b3b3-3173-4596-a077-4581376b324c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecCAYkYYMLei",
        "outputId": "55aef780-14cd-4a7b-d30a-95e17da15fd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7840000"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "X_test.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbvp3joZMOG5",
        "outputId": "d9b3a031-743d-4276-d3da-5561201dda54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100000"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "y_test.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "qm23CzRihaW0"
      },
      "outputs": [],
      "source": [
        "#X_train = np.random.randn(number_of_train_examples , size_input)\n",
        "#y_train = np.random.randn(number_of_train_examples)\n",
        "#X_test = np.random.randn(number_of_test_examples, size_input)\n",
        "#y_test = np.random.randn(number_of_test_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aigqKFFF5BM2"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(16)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb4hOoVbnzSJ"
      },
      "source": [
        "## Build MLP using Eager Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Ht9_qpYipgHw"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    output = tf.nn.softmax(output)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUDFOuNk618X"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "FZPVUu0YDa-_"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdMFAuH18Ve0",
        "outputId": "64e40cf3-f3cd-4834-d566-c10ca6048da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.7399565104166667\n",
            "Number of Epoch = 1 - Accuracy:= 8.040072631835937\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.7281026041666666\n",
            "Number of Epoch = 2 - Accuracy:= 9.590027872721354\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.7266733723958333\n",
            "Number of Epoch = 3 - Accuracy:= 9.783367919921874\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.726030859375\n",
            "Number of Epoch = 4 - Accuracy:= 9.868362426757812\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.7256635416666667\n",
            "Number of Epoch = 5 - Accuracy:= 9.915020751953126\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.7254348307291667\n",
            "Number of Epoch = 6 - Accuracy:= 9.95167744954427\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.7252866536458333\n",
            "Number of Epoch = 7 - Accuracy:= 9.971687825520833\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.7251377604166667\n",
            "Number of Epoch = 8 - Accuracy:= 9.993363444010416\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.7250104166666667\n",
            "Number of Epoch = 9 - Accuracy:= 10.008364868164064\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.7249199869791667\n",
            "Number of Epoch = 10 - Accuracy:= 10.025020345052084\n",
            "\n",
            "Total time taken (in seconds): 835.30\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, a =np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, s=(np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PornJVZGJeZb",
        "outputId": "cb98706a-78ee-4b62-840f-54d1c9b44e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[43495.2]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "print(loss_total_gpu\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqsy5LCVju2A",
        "outputId": "04366ad7-147b-4733-93a1-e3dba580be4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0014\n",
            "10.05 %\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "# for inputs, outputs in test_ds:\n",
        "preds = mlp_on_gpu.forward(X_test)\n",
        "#b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(p=(np.sum(test_loss_total.numpy()) / X_test.shape[0])))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "#List comprehension to map the lambda function across all records of y_true and y_pred\n",
        "y_true = np.array([maxposition(rec) for rec in y_test])\n",
        "y_pred = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(y_true == y_pred)/len(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAoquOxV_IM3",
        "outputId": "c7b29304-7ee0-4dd1-85b5-b26c01f89eb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "tf.Tensor(\n",
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]], shape=(5, 10), dtype=float32)\n",
            "[9 2 1 1 6]\n",
            "[7 7 7 7 7]\n"
          ]
        }
      ],
      "source": [
        "print(y_test[:5])\n",
        "print(preds[:5])\n",
        "\n",
        "print(y_true[:5])\n",
        "print(y_pred[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0vQ1z0AJn6G",
        "outputId": "33284e1a-e1cc-4bdc-b8b0-8c3643fb6e86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.7248715494791667\n",
            "Number of Epoch = 1 - Accuracy:= 10.033356730143229\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.724843359375\n",
            "Number of Epoch = 2 - Accuracy:= 10.038351440429688\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.7248111979166667\n",
            "Number of Epoch = 3 - Accuracy:= 10.043360392252604\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.7247806640625\n",
            "Number of Epoch = 4 - Accuracy:= 10.04168904622396\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.724720703125\n",
            "Number of Epoch = 5 - Accuracy:= 10.04168701171875\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.7246442057291667\n",
            "Number of Epoch = 6 - Accuracy:= 10.045008341471354\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.7245557291666667\n",
            "Number of Epoch = 7 - Accuracy:= 10.06168924967448\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.7244735677083334\n",
            "Number of Epoch = 8 - Accuracy:= 10.063364664713543\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.724370703125\n",
            "Number of Epoch = 9 - Accuracy:= 10.07502950032552\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.7241551432291666\n",
            "Number of Epoch = 10 - Accuracy:= 10.10501708984375\n",
            "\n",
            "Total time taken (in seconds): 622.06\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOoIcG8qMqjj",
        "outputId": "7a74df84-c60a-465c-ea75-53c545e2040a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0014\n",
            "15.299999999999999 %\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "# for inputs, outputs in test_ds:\n",
        "preds = mlp_on_cpu.forward(X_test)\n",
        "#b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "#List comprehension to map the lambda function across all records of y_true and y_pred\n",
        "y_true = np.array([maxposition(rec) for rec in y_test])\n",
        "y_pred = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(y_true == y_pred)/len(y_pred)\n",
        "print(val_acc*100,\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4ifzqosLkPe",
        "outputId": "b09ea4d5-acc6-40c3-8162-e0af48be12a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.7238244791666667\n",
            "Number of Epoch = 1 - Accuracy:= 10.120020548502605\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.7235535807291666\n",
            "Number of Epoch = 2 - Accuracy:= 10.145016479492188\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.7233486979166667\n",
            "Number of Epoch = 3 - Accuracy:= 10.18335673014323\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.7230155598958333\n",
            "Number of Epoch = 4 - Accuracy:= 10.218353271484375\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.7226201171875\n",
            "Number of Epoch = 5 - Accuracy:= 10.240017700195313\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.7217220052083333\n",
            "Number of Epoch = 6 - Accuracy:= 10.341678873697917\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.7209531901041667\n",
            "Number of Epoch = 7 - Accuracy:= 10.413339233398437\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.7200380859375\n",
            "Number of Epoch = 8 - Accuracy:= 10.510025024414062\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.7186962239583333\n",
            "Number of Epoch = 9 - Accuracy:= 10.633359781901042\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.7170250651041666\n",
            "Number of Epoch = 10 - Accuracy:= 10.83333943684896\n",
            "\n",
            "Total time taken (in seconds): 706.62\n"
          ]
        }
      ],
      "source": [
        "#TPU mode\n",
        "mlp_on_tpu = MLP(size_input, size_hidden, size_output, device='tpu')\n",
        "\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvCmsIkjPtgc",
        "outputId": "5da5db83-15ea-4d64-81e5-968413fded2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0013\n",
            "16.06 %\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "# for inputs, outputs in test_ds:\n",
        "preds = mlp_on_tpu.forward(X_test)\n",
        "#b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_tpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "#List comprehension to map the lambda function across all records of y_true and y_pred\n",
        "y_true = np.array([maxposition(rec) for rec in y_test])\n",
        "y_pred = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(y_true == y_pred)/len(y_pred)\n",
        "print(val_acc*100,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkAiBJJlQlGz"
      },
      "source": [
        "Using Dropout Layer to avoid overfitting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "FLdpF1-NDy3i"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output\n",
        "\n",
        "    self.dropout_layer = keras.layers.Dropout(rate=0.5)\n",
        "def call(self, input, training=None):\n",
        "  X_tf = tf.cast(X, dtype=tf.float32)\n",
        "  X_tf = self.dropout_layer(X_tf)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer\n",
        "  what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "  hhat = tf.nn.relu(what)\n",
        "  hhat = self.dropout_layer(hhat, training = training)\n",
        "    #Compute the hidden\n",
        "  what2 = tf.matmul(hhat, self.W2) + self.b2\n",
        "  hhat2 = tf.nn.relu(what2)\n",
        "  hhat2 = self.dropout_layer(hhat2, training = training)\n",
        "    # Compute output\n",
        "  output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "  output = tf.nn.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c06GRBIwQ14r",
        "outputId": "de1aab13-14aa-45f4-fb7a-4aa7fd9dcb59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.334763671875\n",
            "Number of Epoch = 1 - Accuracy:= 9.610038248697917\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.247984619140625\n",
            "Number of Epoch = 2 - Accuracy:= 9.831691487630208\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.20563922526041667\n",
            "Number of Epoch = 3 - Accuracy:= 9.915024820963541\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.2129431640625\n",
            "Number of Epoch = 4 - Accuracy:= 9.96002909342448\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.3347446614583333\n",
            "Number of Epoch = 5 - Accuracy:= 11.095002237955729\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.3733469401041667\n",
            "Number of Epoch = 6 - Accuracy:= 11.784975179036458\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.35632203776041665\n",
            "Number of Epoch = 7 - Accuracy:= 11.528352864583333\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.35210146484375\n",
            "Number of Epoch = 8 - Accuracy:= 11.351681518554688\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.3518001953125\n",
            "Number of Epoch = 9 - Accuracy:= 11.311660766601562\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.36057086588541665\n",
            "Number of Epoch = 10 - Accuracy:= 11.18666280110677\n",
            "\n",
            "Total time taken (in seconds): 638.53\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmsKAa5wP8aI"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbxAHMy6S9Nf",
        "outputId": "0b1030c2-4517-4e2d-8acd-24ed57b208ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0007\n",
            "11.06 %\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "# for inputs, outputs in test_ds:\n",
        "preds = mlp_on_gpu.forward(X_test)\n",
        "#b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "#List comprehension to map the lambda function across all records of y_true and y_pred\n",
        "y_true = np.array([maxposition(rec) for rec in y_test])\n",
        "y_pred = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(y_true == y_pred)/len(y_pred)\n",
        "print(val_acc*100,\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "OGadrIxDSYak"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC89zQMPTS6A"
      },
      "source": [
        "L2 Regularization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w_hauIuQPqC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "1cKmBt309FM_"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 \n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "JIpjU1F9t4Nm"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poFF50TMrP0M",
        "outputId": "20cc7509-8c2e-4048-ef9a-764dc22b359b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.6928215494791666\n",
            "Number of Epoch = 1 - Accuracy:= 13.69834696451823\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.6509770182291666\n",
            "Number of Epoch = 2 - Accuracy:= 18.878346761067707\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.6142603515625\n",
            "Number of Epoch = 3 - Accuracy:= 23.381683349609375\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.5738859375\n",
            "Number of Epoch = 4 - Accuracy:= 28.386651611328123\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.5503410807291667\n",
            "Number of Epoch = 5 - Accuracy:= 31.344968668619792\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.5363466796875\n",
            "Number of Epoch = 6 - Accuracy:= 33.15667928059896\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.5280036458333334\n",
            "Number of Epoch = 7 - Accuracy:= 34.168355305989586\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.5221714192708333\n",
            "Number of Epoch = 8 - Accuracy:= 34.92664794921875\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.5163501627604167\n",
            "Number of Epoch = 9 - Accuracy:= 35.67168375651041\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.5127314127604167\n",
            "Number of Epoch = 10 - Accuracy:= 36.158349609375\n",
            "\n",
            "Total time taken (in seconds): 701.55\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXe-2MENCOjq"
      },
      "source": [
        "## One Step Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw7qM-g59Ei3",
        "outputId": "334f47e4-3809-4ed6-a236-54c17331ab37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0010\n",
            "35.74 %\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "# for inputs, outputs in test_ds:\n",
        "preds = mlp_on_gpu.forward(X_test)\n",
        "#b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "#List comprehension to map the lambda function across all records of y_true and y_pred\n",
        "y_true = np.array([maxposition(rec) for rec in y_test])\n",
        "y_pred = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(y_true == y_pred)/len(y_pred)\n",
        "print(val_acc*100,\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoHiiBK_Xk9M",
        "outputId": "fb8c26f6-7d37-4a32-d86f-6efa8f386bfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.5095127278645833\n",
            "Number of Epoch = 1 - Accuracy:= 36.51162516276042\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.5065806640625\n",
            "Number of Epoch = 2 - Accuracy:= 36.89665934244792\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.5030660807291667\n",
            "Number of Epoch = 3 - Accuracy:= 37.318339029947914\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.5004604817708334\n",
            "Number of Epoch = 4 - Accuracy:= 37.68501383463542\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.4981304361979167\n",
            "Number of Epoch = 5 - Accuracy:= 37.96495361328125\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.4954019856770833\n",
            "Number of Epoch = 6 - Accuracy:= 38.30671793619791\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.4938117838541667\n",
            "Number of Epoch = 7 - Accuracy:= 38.49336751302083\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.49145784505208334\n",
            "Number of Epoch = 8 - Accuracy:= 38.81832275390625\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.49015260416666667\n",
            "Number of Epoch = 9 - Accuracy:= 38.988346354166666\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.48873583984375\n",
            "Number of Epoch = 10 - Accuracy:= 39.12168782552084\n",
            "\n",
            "Total time taken (in seconds): 717.63\n"
          ]
        }
      ],
      "source": [
        "#TPU mode\n",
        "mlp_on_tpu = MLP(size_input, size_hidden, size_output, device='tpu')\n",
        "\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUxMw0BYXnVk",
        "outputId": "85de67a2-46a4-40ce-a403-b7f122649054"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0014\n",
            "9.969999999999999 %\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "# for inputs, outputs in test_ds:\n",
        "preds = mlp_on_tpu.forward(X_test)\n",
        "#b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_tpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "#List comprehension to map the lambda function across all records of y_true and y_pred\n",
        "y_true = np.array([maxposition(rec) for rec in y_test])\n",
        "y_pred = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(y_true == y_pred)/len(y_pred)\n",
        "print(val_acc*100,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-ujvvwdXBbA"
      },
      "source": [
        "Hyperparameter Optimization; Since I am getting better accuracy with the L2 Regularization, therefore I am going to perform hyperparamete tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "huHv3m5UXIz9"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 \n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.leaky_relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.leaky_relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxZQlCWSddr2",
        "outputId": "4c57ef9e-5f6b-4521-9b9c-cf49a949e857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.769164453125\n",
            "Number of Epoch = 1 - Accuracy:= 4.385030619303385\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.760647265625\n",
            "Number of Epoch = 2 - Accuracy:= 5.4500579833984375\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.752294921875\n",
            "Number of Epoch = 3 - Accuracy:= 6.493408203125001\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.7465244140625\n",
            "Number of Epoch = 4 - Accuracy:= 7.243416849772136\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.741366796875\n",
            "Number of Epoch = 5 - Accuracy:= 7.901751200358073\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.7337123046875\n",
            "Number of Epoch = 6 - Accuracy:= 8.81007080078125\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.6949836588541667\n",
            "Number of Epoch = 7 - Accuracy:= 13.588326009114585\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.6784076822916667\n",
            "Number of Epoch = 8 - Accuracy:= 15.711690266927084\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.6743356119791667\n",
            "Number of Epoch = 9 - Accuracy:= 16.20833943684896\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.6710272135416666\n",
            "Number of Epoch = 10 - Accuracy:= 16.639992268880206\n",
            "\n",
            "Total time taken (in seconds): 701.75\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VppgKSHi9Vt"
      },
      "source": [
        "2. Changing the Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "VZNRzpiUiHbW"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.05)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 \n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.leaky_relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.leaky_relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fhSiMhjjFnu",
        "outputId": "866073b4-6f50-4021-d2b5-a9ba8899a555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.7256953776041667\n",
            "Number of Epoch = 1 - Accuracy:= 9.951706949869791\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.7253164713541667\n",
            "Number of Epoch = 2 - Accuracy:= 10.00004374186198\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.7253152994791666\n",
            "Number of Epoch = 3 - Accuracy:= 10.000034586588542\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.7253160807291666\n",
            "Number of Epoch = 4 - Accuracy:= 10.0000244140625\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.7253161458333334\n",
            "Number of Epoch = 5 - Accuracy:= 10.000014241536459\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.7253158203125\n",
            "Number of Epoch = 6 - Accuracy:= 10.000023396809896\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.7253161458333334\n",
            "Number of Epoch = 7 - Accuracy:= 10.00003662109375\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.7253166015625\n",
            "Number of Epoch = 8 - Accuracy:= 10.000038655598958\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.725316796875\n",
            "Number of Epoch = 9 - Accuracy:= 10.000042724609374\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.7253155598958333\n",
            "Number of Epoch = 10 - Accuracy:= 10.00003662109375\n",
            "\n",
            "Total time taken (in seconds): 742.21\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O_MDQhRltll",
        "outputId": "01a71824-b2ac-44f7-de95-1a5d3a369943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0015\n",
            "10.0 %\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "# for inputs, outputs in test_ds:\n",
        "preds = mlp_on_gpu.forward(X_test)\n",
        "#b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "#List comprehension to map the lambda function across all records of y_true and y_pred\n",
        "y_true = np.array([maxposition(rec) for rec in y_test])\n",
        "y_pred = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(y_true == y_pred)/len(y_pred)\n",
        "print(val_acc*100,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI80_PC2mFvn"
      },
      "source": [
        "Changing Activation function to Relu instead Leaky_Relu and changing the learnin rate to 0.6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "8Im-DzlcmE6t"
      },
      "outputs": [],
      "source": [
        "# Initialize model using GPU\n",
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.06)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 \n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zpc3LxQlmqOx",
        "outputId": "61d4a961-8bec-42c4-e900-2af9531dca5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.70774296875\n",
            "Number of Epoch = 1 - Accuracy:= 12.168369547526042\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.7034765625\n",
            "Number of Epoch = 2 - Accuracy:= 12.706581624348958\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.6640889322916667\n",
            "Number of Epoch = 3 - Accuracy:= 17.58834431966146\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.6667590494791666\n",
            "Number of Epoch = 4 - Accuracy:= 17.26332804361979\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.662168359375\n",
            "Number of Epoch = 5 - Accuracy:= 17.823368326822916\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.6736038411458334\n",
            "Number of Epoch = 6 - Accuracy:= 16.37663879394531\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.7253161458333334\n",
            "Number of Epoch = 7 - Accuracy:= 10.00003662109375\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.7253166015625\n",
            "Number of Epoch = 8 - Accuracy:= 10.000038655598958\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.725316796875\n",
            "Number of Epoch = 9 - Accuracy:= 10.000042724609374\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.7253155598958333\n",
            "Number of Epoch = 10 - Accuracy:= 10.00003662109375\n",
            "\n",
            "Total time taken (in seconds): 724.68\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwgVUAABmy_M",
        "outputId": "7f6edc19-7014-47df-c6ad-6109b9c14335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0015\n",
            "10.0 %\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "# for inputs, outputs in test_ds:\n",
        "preds = mlp_on_gpu.forward(X_test)\n",
        "#b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "#List comprehension to map the lambda function across all records of y_true and y_pred\n",
        "y_true = np.array([maxposition(rec) for rec in y_test])\n",
        "y_pred = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(y_true == y_pred)/len(y_pred)\n",
        "print(val_acc*100,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzXhba09qlkS"
      },
      "source": [
        "4. Lets increase the epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAaOQepTqeQT",
        "outputId": "b99d5731-a0ba-4d52-a13a-1d6bd261cef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.7252026041666667\n",
            "Number of Epoch = 1 - Accuracy:= 10.008348592122395\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.7253169921875\n",
            "Number of Epoch = 2 - Accuracy:= 10.000028483072915\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.7253162109375\n",
            "Number of Epoch = 3 - Accuracy:= 10.000015258789062\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.7253164713541667\n",
            "Number of Epoch = 4 - Accuracy:= 10.000020345052084\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.7253154947916667\n",
            "Number of Epoch = 5 - Accuracy:= 10.0000244140625\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.7253171223958333\n",
            "Number of Epoch = 6 - Accuracy:= 10.000031534830729\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.7253168619791667\n",
            "Number of Epoch = 7 - Accuracy:= 10.000022379557292\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.7253160807291666\n",
            "Number of Epoch = 8 - Accuracy:= 10.000015258789062\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.7253158854166667\n",
            "Number of Epoch = 9 - Accuracy:= 10.000020345052084\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.7253157552083334\n",
            "Number of Epoch = 10 - Accuracy:= 10.000022379557292\n",
            "Number of Epoch = 11 - Categorical Cross-Entropy:= 0.36305979817708334\n",
            "Number of Epoch = 11 - Accuracy:= 44.918359375\n",
            "Number of Epoch = 12 - Categorical Cross-Entropy:= 0.023209415690104165\n",
            "Number of Epoch = 12 - Accuracy:= 83.11842447916666\n",
            "Number of Epoch = 13 - Categorical Cross-Entropy:= 0.01999208780924479\n",
            "Number of Epoch = 13 - Accuracy:= 85.35330403645833\n",
            "Number of Epoch = 14 - Categorical Cross-Entropy:= 0.01837477010091146\n",
            "Number of Epoch = 14 - Accuracy:= 86.55319010416666\n",
            "Number of Epoch = 15 - Categorical Cross-Entropy:= 0.01736180216471354\n",
            "Number of Epoch = 15 - Accuracy:= 87.16306966145834\n",
            "\n",
            "Total time taken (in seconds): 1046.46\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 15\n",
        "\n",
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrhQbH38q5Kl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdfjh37sq5xP",
        "outputId": "03833141-b206-4bd1-f3f1-49d39ddc048d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0000\n",
            "85.45 %\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "# for inputs, outputs in test_ds:\n",
        "preds = mlp_on_gpu.forward(X_test)\n",
        "#b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "#List comprehension to map the lambda function across all records of y_true and y_pred\n",
        "y_true = np.array([maxposition(rec) for rec in y_test])\n",
        "y_pred = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(y_true == y_pred)/len(y_pred)\n",
        "print(val_acc*100,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D1lIS75vRbZ"
      },
      "source": [
        "5. Let's try L1 regularization with No. of epochs as 12 and learning rate as 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "-UgPKNzCvrI2"
      },
      "outputs": [],
      "source": [
        "# Initialize model using GPU\n",
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.06)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L1= (tf.reduce_sum(self.W1)+ tf.reduce_sum(self.W2)+tf.reduce_sum(self.W3)) \n",
        "      current_loss = self.loss(predicted, y_train) + 0.01 * L1 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBH3WVrVzeJQ",
        "outputId": "b442d7ee-1c1c-4595-bf45-d291bf3f7558"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.15945930989583335\n",
            "Number of Epoch = 1 - Accuracy:= 10.943347167968751\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.1171131103515625\n",
            "Number of Epoch = 2 - Accuracy:= 9.878368123372395\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.11654532063802084\n",
            "Number of Epoch = 3 - Accuracy:= 9.990025838216146\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.11627923990885417\n",
            "Number of Epoch = 4 - Accuracy:= 9.863367716471354\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.11606238606770833\n",
            "Number of Epoch = 5 - Accuracy:= 9.89502156575521\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.11585870768229167\n",
            "Number of Epoch = 6 - Accuracy:= 9.936688232421876\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.1156848876953125\n",
            "Number of Epoch = 7 - Accuracy:= 9.92504170735677\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.11560489908854167\n",
            "Number of Epoch = 8 - Accuracy:= 9.953370157877604\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.11554691569010417\n",
            "Number of Epoch = 9 - Accuracy:= 10.120030721028646\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.11546092122395833\n",
            "Number of Epoch = 10 - Accuracy:= 10.136684163411458\n",
            "Number of Epoch = 11 - Categorical Cross-Entropy:= 0.115413134765625\n",
            "Number of Epoch = 11 - Accuracy:= 10.176678466796876\n",
            "Number of Epoch = 12 - Categorical Cross-Entropy:= 0.11540289713541667\n",
            "Number of Epoch = 12 - Accuracy:= 10.10168965657552\n",
            "\n",
            "Total time taken (in seconds): 844.18\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 12\n",
        "\n",
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtBHz7rJziSz",
        "outputId": "b6b66bac-2b64-48d7-b846-643174672148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0002\n",
            "10.0 %\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "# for inputs, outputs in test_ds:\n",
        "preds = mlp_on_gpu.forward(X_test)\n",
        "#b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "#List comprehension to map the lambda function across all records of y_true and y_pred\n",
        "y_true = np.array([maxposition(rec) for rec in y_test])\n",
        "y_pred = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(y_true == y_pred)/len(y_pred)\n",
        "print(val_acc*100,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA9dTQhAPpJA"
      },
      "source": [
        "6. Taking L2 regularization, learning rate = 0.6 and epochs as 12\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "j6NBTQWy3sUf"
      },
      "outputs": [],
      "source": [
        "# Initialize model using GPU\n",
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.06)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 \n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "FBrlDF6T3x7V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a090137-910d-4196-857c-0c51a0f2af74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.7235477213541667\n",
            "Number of Epoch = 1 - Accuracy:= 10.216709391276042\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.7253298828125\n",
            "Number of Epoch = 2 - Accuracy:= 9.998377482096355\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.7253248697916667\n",
            "Number of Epoch = 3 - Accuracy:= 9.998367309570312\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.7253160807291666\n",
            "Number of Epoch = 4 - Accuracy:= 10.0000244140625\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.7253161458333334\n",
            "Number of Epoch = 5 - Accuracy:= 10.000014241536459\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.7253158203125\n",
            "Number of Epoch = 6 - Accuracy:= 10.000023396809896\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.7253161458333334\n",
            "Number of Epoch = 7 - Accuracy:= 10.00003662109375\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.7253166015625\n",
            "Number of Epoch = 8 - Accuracy:= 10.000038655598958\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.725316796875\n",
            "Number of Epoch = 9 - Accuracy:= 10.000042724609374\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.7253155598958333\n",
            "Number of Epoch = 10 - Accuracy:= 10.00003662109375\n",
            "Number of Epoch = 11 - Categorical Cross-Entropy:= 0.72531640625\n",
            "Number of Epoch = 11 - Accuracy:= 10.000023396809896\n",
            "Number of Epoch = 12 - Categorical Cross-Entropy:= 0.7253163411458333\n",
            "Number of Epoch = 12 - Accuracy:= 10.000034586588542\n",
            "\n",
            "Total time taken (in seconds): 884.00\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 12\n",
        "\n",
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxNp8f_aQG0c"
      },
      "source": [
        "7.Changing the optimizer to adam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "zzbQB4sBQPeM"
      },
      "outputs": [],
      "source": [
        "# Initialize model using GPU\n",
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.06)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 \n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "FfOrwK9_Qabx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c759550-e924-42a6-f207-6e9e3949a23b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.12183611653645833\n",
            "Number of Epoch = 1 - Accuracy:= 39.02999267578125\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.14462482096354168\n",
            "Number of Epoch = 2 - Accuracy:= 44.873347981770834\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.20283055013020834\n",
            "Number of Epoch = 3 - Accuracy:= 45.629964192708336\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.28638326822916665\n",
            "Number of Epoch = 4 - Accuracy:= 37.92996826171875\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.47443805338541667\n",
            "Number of Epoch = 5 - Accuracy:= 25.369913736979164\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.6531487630208334\n",
            "Number of Epoch = 6 - Accuracy:= 10.03669942220052\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.6521241536458333\n",
            "Number of Epoch = 7 - Accuracy:= 9.8616943359375\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.6517583984375\n",
            "Number of Epoch = 8 - Accuracy:= 9.861690266927084\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.6517572265625\n",
            "Number of Epoch = 9 - Accuracy:= 9.941682942708335\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.6517736979166666\n",
            "Number of Epoch = 10 - Accuracy:= 9.923360188802082\n",
            "Number of Epoch = 11 - Categorical Cross-Entropy:= 0.6517517578125\n",
            "Number of Epoch = 11 - Accuracy:= 9.918379720052084\n",
            "Number of Epoch = 12 - Categorical Cross-Entropy:= 0.6517575520833333\n",
            "Number of Epoch = 12 - Accuracy:= 9.920035807291667\n",
            "\n",
            "Total time taken (in seconds): 1432.06\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 12\n",
        "\n",
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajfISMFnXb19"
      },
      "source": [
        "8. Changing the batch size and changing the optimizer back to SGD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "l6EZ2QskVfjh"
      },
      "outputs": [],
      "source": [
        "# Initialize model using GPU\n",
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.06)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 \n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "COG3k7NcXu2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18fe69f4-6cec-49d4-fd98-3ba80eb27d89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 1.4506947916666666\n",
            "Number of Epoch = 1 - Accuracy:= 9.995091756184896\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 1.4506130208333334\n",
            "Number of Epoch = 2 - Accuracy:= 10.000094604492189\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.4505200520833332\n",
            "Number of Epoch = 3 - Accuracy:= 9.996755981445313\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.4497763020833334\n",
            "Number of Epoch = 4 - Accuracy:= 10.015072631835938\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.23533079427083334\n",
            "Number of Epoch = 5 - Accuracy:= 15.929986572265625\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.163135986328125\n",
            "Number of Epoch = 6 - Accuracy:= 35.08503824869791\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.13875437825520834\n",
            "Number of Epoch = 7 - Accuracy:= 45.773441569010416\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.1262797607421875\n",
            "Number of Epoch = 8 - Accuracy:= 50.685087076822924\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.11966914876302083\n",
            "Number of Epoch = 9 - Accuracy:= 53.74170735677083\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.11596357421875\n",
            "Number of Epoch = 10 - Accuracy:= 55.53504231770834\n",
            "Number of Epoch = 11 - Categorical Cross-Entropy:= 0.11285923665364583\n",
            "Number of Epoch = 11 - Accuracy:= 56.51183675130208\n",
            "Number of Epoch = 12 - Categorical Cross-Entropy:= 0.11176333821614583\n",
            "Number of Epoch = 12 - Accuracy:= 57.28498942057292\n",
            "\n",
            "Total time taken (in seconds): 1482.44\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 12\n",
        "\n",
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(10)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 10 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhu9Zfh-sXj9"
      },
      "source": [
        "Changing the batch size to 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Ed6VF_SbivfA"
      },
      "outputs": [],
      "source": [
        "# Initialize model using GPU\n",
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.06)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 \n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "PxQMY4ICi3Hr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0af55d8-b073-40e2-97e0-3b6c9ba632aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.48350338541666665\n",
            "Number of Epoch = 1 - Accuracy:= 10.003385925292969\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.48353658854166665\n",
            "Number of Epoch = 2 - Accuracy:= 10.000033569335939\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.48353766276041665\n",
            "Number of Epoch = 3 - Accuracy:= 9.996703338623048\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.4835364583333333\n",
            "Number of Epoch = 4 - Accuracy:= 10.000030517578125\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.48353642578125\n",
            "Number of Epoch = 5 - Accuracy:= 10.000029754638671\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.48353658854166665\n",
            "Number of Epoch = 6 - Accuracy:= 10.000029754638671\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.48353658854166665\n",
            "Number of Epoch = 7 - Accuracy:= 10.000033569335939\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.483476953125\n",
            "Number of Epoch = 8 - Accuracy:= 9.985028839111328\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.4835364583333333\n",
            "Number of Epoch = 9 - Accuracy:= 10.000035858154297\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.4835364583333333\n",
            "Number of Epoch = 10 - Accuracy:= 10.000029754638671\n",
            "Number of Epoch = 11 - Categorical Cross-Entropy:= 0.32269524739583333\n",
            "Number of Epoch = 11 - Accuracy:= 10.911682891845704\n",
            "Number of Epoch = 12 - Categorical Cross-Entropy:= 0.07352332356770834\n",
            "Number of Epoch = 12 - Accuracy:= 16.165052795410155\n",
            "\n",
            "Total time taken (in seconds): 490.84\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 12\n",
        "\n",
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(30)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 30 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "k7XVbcnhuFv1"
      },
      "outputs": [],
      "source": [
        "# Initialize model using GPU\n",
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.06)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 \n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "KLvewF9RuMfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fffbfafb-077f-4a7e-c2d8-75e4c9b60fd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.4835365234375\n",
            "Number of Epoch = 1 - Accuracy:= 10.000051879882813\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.48353658854166665\n",
            "Number of Epoch = 2 - Accuracy:= 10.000033569335939\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.4835365234375\n",
            "Number of Epoch = 3 - Accuracy:= 10.00003662109375\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.4835364583333333\n",
            "Number of Epoch = 4 - Accuracy:= 10.000030517578125\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.48353642578125\n",
            "Number of Epoch = 5 - Accuracy:= 10.000029754638671\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.48353658854166665\n",
            "Number of Epoch = 6 - Accuracy:= 10.000029754638671\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.48353658854166665\n",
            "Number of Epoch = 7 - Accuracy:= 10.000033569335939\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.48316637369791665\n",
            "Number of Epoch = 8 - Accuracy:= 10.045030975341797\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.4835366861979167\n",
            "Number of Epoch = 9 - Accuracy:= 10.000032806396485\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.4835368489583333\n",
            "Number of Epoch = 10 - Accuracy:= 10.00003890991211\n",
            "Number of Epoch = 11 - Categorical Cross-Entropy:= 0.14333232421875\n",
            "Number of Epoch = 11 - Accuracy:= 13.36670379638672\n",
            "Number of Epoch = 12 - Categorical Cross-Entropy:= 0.06911964518229166\n",
            "Number of Epoch = 12 - Accuracy:= 21.0683837890625\n",
            "\n",
            "Total time taken (in seconds): 490.47\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 12\n",
        "\n",
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(30)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 30 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "7bXBhwWh2jcm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7b9f684-73d0-4e62-b1fc-f576bd831941"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0002\n",
            "37.05 %\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "# for inputs, outputs in test_ds:\n",
        "preds = mlp_on_gpu.forward(X_test)\n",
        "#b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "#List comprehension to map the lambda function across all records of y_true and y_pred\n",
        "y_true = np.array([maxposition(rec) for rec in y_test])\n",
        "y_pred = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(y_true == y_pred)/len(y_pred)\n",
        "print(val_acc*100,\"%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Fashion_mnist",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}