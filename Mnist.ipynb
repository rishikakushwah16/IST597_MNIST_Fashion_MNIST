{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71kdFp0QgF4K"
      },
      "source": [
        "# IST597:- Multi-Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2yHcl5xgPV1"
      },
      "source": [
        "## Load the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "2DPwxLR2gSLC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "np.random.seed(5510)\n",
        "tf.random.set_seed(5510)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV-3kEaggcO8",
        "outputId": "09bca84c-bf05-478e-9ffe-0dd7db025595"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw78jw6pDqSM"
      },
      "source": [
        "#Get number of Gpu's and id's in the system or else you can also use Nvidia-smi in command prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dk_S2TMg_6_"
      },
      "source": [
        "## Generate random data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "40XlFnwho7D8"
      },
      "outputs": [],
      "source": [
        "size_input = 784\n",
        "size_hidden =[128, 64]\n",
        "size_output = 10\n",
        "number_of_train_examples = 60000\n",
        "number_of_test_examples = 10000\n",
        "from tensorflow.keras.datasets import mnist\n",
        "# load dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpRqHa3Dq3-X",
        "outputId": "b894e77b-85db-4c7a-bea3-c7329bf7f9ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "RhCEG3fcrP-w"
      },
      "outputs": [],
      "source": [
        "y_train = tf.keras.utils.to_categorical(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "GL7kLRPl0VLr"
      },
      "outputs": [],
      "source": [
        "y_test = tf.keras.utils.to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMTdXC1SrSgQ",
        "outputId": "79c11a05-0b5a-4f93-e844-1b602f911c1d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ],
      "source": [
        "y_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "YtNd_X5M9Iuz"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.reshape(60000, 28*28)\n",
        "X_test = X_test.reshape(10000, 28*28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_hEngtyr1o2",
        "outputId": "94000fb6-333b-4af4-e75f-3fa252f46a82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u9thOVXr-E_",
        "outputId": "235d4cb8-4ee3-4344-a78d-bce9944fd472"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "EOm4zNP49mVZ"
      },
      "outputs": [],
      "source": [
        "X_train = X_train / 255\n",
        "X_test = X_test / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6LRQZJoFtJK",
        "outputId": "a28ba55a-f28b-447e-f0c7-a6ce1e663227"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzNCoAuXSY7E",
        "outputId": "3d04e11e-c04a-4ed9-ed98-e10d225353e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGn2kjp7UOC9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYJT5X0CUOoc",
        "outputId": "a17974fe-039c-49dd-84a3-d4977ce3a037"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO7-is5ALn-r",
        "outputId": "0791270b-c57b-46f6-880c-9ce3a376c194"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47040000"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ],
      "source": [
        "X_train.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1egEb3sLoG7",
        "outputId": "86ef938e-9bdd-4a26-d079-8f5300f8cc41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600000"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ],
      "source": [
        "y_train.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-unAB1UeUTAc",
        "outputId": "a5dd604d-ba8c-4c58-9e55-81d26835e513"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecCAYkYYMLei",
        "outputId": "66a676df-60b2-4623-c94f-6e16d0045e79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7840000"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ],
      "source": [
        "X_test.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbvp3joZMOG5",
        "outputId": "bbea6854-67c7-42db-eec6-5cd0dcf95eb6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100000"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ],
      "source": [
        "y_test.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qm23CzRihaW0"
      },
      "outputs": [],
      "source": [
        "#X_train = np.random.randn(number_of_train_examples , size_input)\n",
        "#y_train = np.random.randn(number_of_train_examples)\n",
        "#X_test = np.random.randn(number_of_test_examples, size_input)\n",
        "#y_test = np.random.randn(number_of_test_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "aigqKFFF5BM2"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(16)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb4hOoVbnzSJ"
      },
      "source": [
        "## Build MLP using Eager Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht9_qpYipgHw"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    output = tf.nn.softmax(output)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUDFOuNk618X"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZPVUu0YDa-_"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdMFAuH18Ve0",
        "outputId": "65315371-805d-4a8c-edcf-8c7597e24c09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.7146720703125\n",
            "Number of Epoch = 1 - Accuracy:= 10.915017700195312\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.6942576822916666\n",
            "Number of Epoch = 2 - Accuracy:= 13.48499247233073\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.678141015625\n",
            "Number of Epoch = 3 - Accuracy:= 15.509998575846353\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.6642876953125\n",
            "Number of Epoch = 4 - Accuracy:= 17.191678873697917\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.6521606119791666\n",
            "Number of Epoch = 5 - Accuracy:= 18.74502970377604\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.6392409505208333\n",
            "Number of Epoch = 6 - Accuracy:= 20.266695149739583\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.6273642578125\n",
            "Number of Epoch = 7 - Accuracy:= 21.786692301432293\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.6180056640625\n",
            "Number of Epoch = 8 - Accuracy:= 22.96668904622396\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.6090388671875\n",
            "Number of Epoch = 9 - Accuracy:= 24.073368326822916\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.6011380859375\n",
            "Number of Epoch = 10 - Accuracy:= 25.030027262369792\n",
            "\n",
            "Total time taken (in seconds): 602.84\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PornJVZGJeZb",
        "outputId": "2ac2fe8c-f41e-44b2-fc56-256c67e6b318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[36068.285]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "print(loss_total_gpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqsy5LCVju2A",
        "outputId": "e18b7342-8a60-4305-e90d-26d77c135843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0012\n",
            "24.89 %\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "# for inputs, outputs in test_ds:\n",
        "preds = mlp_on_gpu.forward(X_test)\n",
        "#b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "#List comprehension to map the lambda function across all records of y_true and y_pred\n",
        "y_true = np.array([maxposition(rec) for rec in y_test])\n",
        "y_pred = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(y_true == y_pred)/len(y_pred)\n",
        "print(val_acc*100,\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAoquOxV_IM3",
        "outputId": "5bef0aac-a96d-4561-8685-e4ae48a82e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "tf.Tensor(\n",
            "[[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
            " [0.0000000e+00 8.4785869e-28 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 4.0407927e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
            " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
            " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  4.4565869e-18 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
            " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]], shape=(5, 10), dtype=float32)\n",
            "[7 2 1 0 4]\n",
            "[7 7 7 6 6]\n"
          ]
        }
      ],
      "source": [
        "print(y_test[:5])\n",
        "print(preds[:5])\n",
        "\n",
        "print(y_true[:5])\n",
        "print(y_pred[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkAiBJJlQlGz"
      },
      "source": [
        "Using Dropout Layer to avoid overfitting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLdpF1-NDy3i"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output\n",
        "\n",
        "    self.dropout_layer = keras.layers.Dropout(rate=0.5)\n",
        "def call(self, input, training=None):\n",
        "  X_tf = tf.cast(X, dtype=tf.float32)\n",
        "  X_tf = self.dropout_layer(X_tf)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer\n",
        "  what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "  hhat = tf.nn.relu(what)\n",
        "  hhat = self.dropout_layer(hhat, training = training)\n",
        "    #Compute the hidden\n",
        "  what2 = tf.matmul(hhat, self.W2) + self.b2\n",
        "  hhat2 = tf.nn.relu(what2)\n",
        "  hhat2 = self.dropout_layer(hhat2, training = training)\n",
        "    # Compute output\n",
        "  output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "  output = tf.nn.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c06GRBIwQ14r"
      },
      "outputs": [],
      "source": [
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmsKAa5wP8aI"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbxAHMy6S9Nf"
      },
      "outputs": [],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "# for inputs, outputs in test_ds:\n",
        "preds = mlp_on_gpu.forward(X_test)\n",
        "#b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "#List comprehension to map the lambda function across all records of y_true and y_pred\n",
        "y_true = np.array([maxposition(rec) for rec in y_test])\n",
        "y_pred = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(y_true == y_pred)/len(y_pred)\n",
        "print(val_acc*100,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC89zQMPTS6A"
      },
      "source": [
        "L2 Regularization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w_hauIuQPqC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cKmBt309FM_"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 \n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIpjU1F9t4Nm"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poFF50TMrP0M",
        "outputId": "4c46801e-f353-4382-9c53-3807c9fc5ff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.7026961588541667\n",
            "Number of Epoch = 1 - Accuracy:= 12.421675618489584\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.6622991536458334\n",
            "Number of Epoch = 2 - Accuracy:= 17.348350016276044\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.6262250651041666\n",
            "Number of Epoch = 3 - Accuracy:= 21.835028076171874\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.6029328125\n",
            "Number of Epoch = 4 - Accuracy:= 24.751688639322918\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.5882063802083334\n",
            "Number of Epoch = 5 - Accuracy:= 26.61334228515625\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.5769110677083333\n",
            "Number of Epoch = 6 - Accuracy:= 28.058341471354165\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.5672538411458333\n",
            "Number of Epoch = 7 - Accuracy:= 29.258294677734376\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.5578458984375\n",
            "Number of Epoch = 8 - Accuracy:= 30.42334798177083\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.5474572265625\n",
            "Number of Epoch = 9 - Accuracy:= 31.70831705729167\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.5386317057291666\n",
            "Number of Epoch = 10 - Accuracy:= 32.81327311197917\n",
            "\n",
            "Total time taken (in seconds): 819.93\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXe-2MENCOjq"
      },
      "source": [
        "## One Step Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw7qM-g59Ei3",
        "outputId": "4f17b0f5-e93b-4532-f917-38bc0545087e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0011\n",
            "33.15 %\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "# for inputs, outputs in test_ds:\n",
        "preds = mlp_on_gpu.forward(X_test)\n",
        "#b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "#List comprehension to map the lambda function across all records of y_true and y_pred\n",
        "y_true = np.array([maxposition(rec) for rec in y_test])\n",
        "y_pred = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(y_true == y_pred)/len(y_pred)\n",
        "print(val_acc*100,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-ujvvwdXBbA"
      },
      "source": [
        "Hyperparameter Optimization; Since I am getting better accuracy with the L2 Regularization, therefore I am going to perform hyperparamete tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huHv3m5UXIz9"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 \n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.leaky_relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.leaky_relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxZQlCWSddr2",
        "outputId": "bdb1bac9-2a1f-41e5-a156-69eb8a7bb9aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.694659765625\n",
            "Number of Epoch = 1 - Accuracy:= 13.3783447265625\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.6614013671875\n",
            "Number of Epoch = 2 - Accuracy:= 17.473345947265624\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.6340060546875\n",
            "Number of Epoch = 3 - Accuracy:= 20.808378092447917\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.6094662760416667\n",
            "Number of Epoch = 4 - Accuracy:= 23.876688639322914\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.5864924479166667\n",
            "Number of Epoch = 5 - Accuracy:= 26.75334269205729\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.5666244791666667\n",
            "Number of Epoch = 6 - Accuracy:= 29.216674804687496\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.5495546223958333\n",
            "Number of Epoch = 7 - Accuracy:= 31.321638997395834\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.53460498046875\n",
            "Number of Epoch = 8 - Accuracy:= 33.161659749348956\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.5200240885416667\n",
            "Number of Epoch = 9 - Accuracy:= 34.99333089192708\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.5090932942708334\n",
            "Number of Epoch = 10 - Accuracy:= 36.386645507812496\n",
            "\n",
            "Total time taken (in seconds): 837.83\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VppgKSHi9Vt"
      },
      "source": [
        "2. Changing the Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZNRzpiUiHbW"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.05)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 \n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.leaky_relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.leaky_relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fhSiMhjjFnu",
        "outputId": "be5beef5-962b-4f21-ff9c-d7e53045e2b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.5683600260416667\n",
            "Number of Epoch = 1 - Accuracy:= 29.41667073567708\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.44155390625\n",
            "Number of Epoch = 2 - Accuracy:= 45.15502115885416\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.40423111979166665\n",
            "Number of Epoch = 3 - Accuracy:= 49.7633056640625\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.33054612630208335\n",
            "Number of Epoch = 4 - Accuracy:= 58.80325927734374\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.30035400390625\n",
            "Number of Epoch = 5 - Accuracy:= 62.60998942057292\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.29451624348958333\n",
            "Number of Epoch = 6 - Accuracy:= 63.325008138020834\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.28211653645833334\n",
            "Number of Epoch = 7 - Accuracy:= 64.85165201822916\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.27899697265625\n",
            "Number of Epoch = 8 - Accuracy:= 65.21839599609375\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.2580663411458333\n",
            "Number of Epoch = 9 - Accuracy:= 67.66337076822917\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.197308642578125\n",
            "Number of Epoch = 10 - Accuracy:= 75.18666178385416\n",
            "\n",
            "Total time taken (in seconds): 689.83\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O_MDQhRltll",
        "outputId": "120bcb17-7444-4d74-eba7-d3a087325559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0004\n",
            "76.02 %\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "# for inputs, outputs in test_ds:\n",
        "preds = mlp_on_gpu.forward(X_test)\n",
        "#b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "#List comprehension to map the lambda function across all records of y_true and y_pred\n",
        "y_true = np.array([maxposition(rec) for rec in y_test])\n",
        "y_pred = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(y_true == y_pred)/len(y_pred)\n",
        "print(val_acc*100,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI80_PC2mFvn"
      },
      "source": [
        "Changing Activation function to Relu instead Leaky_Relu and changing the learnin rate to 0.6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Im-DzlcmE6t"
      },
      "outputs": [],
      "source": [
        "# Initialize model using GPU\n",
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.06)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 \n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zpc3LxQlmqOx",
        "outputId": "f260a6cd-57ab-4dfd-8f50-95f7d010cfac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.6267244791666666\n",
            "Number of Epoch = 1 - Accuracy:= 22.15669962565104\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.39377301432291667\n",
            "Number of Epoch = 2 - Accuracy:= 51.01829427083333\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.3199173828125\n",
            "Number of Epoch = 3 - Accuracy:= 60.20001220703125\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.29841015625\n",
            "Number of Epoch = 4 - Accuracy:= 62.88002115885417\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.29119697265625\n",
            "Number of Epoch = 5 - Accuracy:= 63.78494873046875\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.27932350260416666\n",
            "Number of Epoch = 6 - Accuracy:= 65.24838053385416\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.273338671875\n",
            "Number of Epoch = 7 - Accuracy:= 65.96993001302084\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.221247802734375\n",
            "Number of Epoch = 8 - Accuracy:= 72.24005533854167\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.19507179361979166\n",
            "Number of Epoch = 9 - Accuracy:= 75.47343749999999\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.13820533854166667\n",
            "Number of Epoch = 10 - Accuracy:= 80.11971842447917\n",
            "\n",
            "Total time taken (in seconds): 758.28\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzXhba09qlkS"
      },
      "source": [
        "4. Lets increase the epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAaOQepTqeQT",
        "outputId": "7cecba80-b438-4ef7-93ea-c4273cf3d620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.72158359375\n",
            "Number of Epoch = 1 - Accuracy:= 10.460022989908854\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.7217572265625\n",
            "Number of Epoch = 2 - Accuracy:= 10.441695149739584\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.7217569010416667\n",
            "Number of Epoch = 3 - Accuracy:= 10.441706339518229\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.72175703125\n",
            "Number of Epoch = 4 - Accuracy:= 10.441696166992188\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.7217568359375\n",
            "Number of Epoch = 5 - Accuracy:= 10.441683959960937\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.7217577473958333\n",
            "Number of Epoch = 6 - Accuracy:= 10.441691080729166\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.50200693359375\n",
            "Number of Epoch = 7 - Accuracy:= 34.16334228515625\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.017688210042317708\n",
            "Number of Epoch = 8 - Accuracy:= 90.0225341796875\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.010405260213216146\n",
            "Number of Epoch = 9 - Accuracy:= 93.76220703125\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.007701311747233073\n",
            "Number of Epoch = 10 - Accuracy:= 95.37894694010417\n",
            "Number of Epoch = 11 - Categorical Cross-Entropy:= 0.006217521667480469\n",
            "Number of Epoch = 11 - Accuracy:= 96.19056803385416\n",
            "Number of Epoch = 12 - Categorical Cross-Entropy:= 0.00521871592203776\n",
            "Number of Epoch = 12 - Accuracy:= 96.89729817708333\n",
            "Number of Epoch = 13 - Categorical Cross-Entropy:= 0.004507203165690104\n",
            "Number of Epoch = 13 - Accuracy:= 97.30569661458334\n",
            "Number of Epoch = 14 - Categorical Cross-Entropy:= 0.003963622792561849\n",
            "Number of Epoch = 14 - Accuracy:= 97.68080240885416\n",
            "Number of Epoch = 15 - Categorical Cross-Entropy:= 0.0035662109375\n",
            "Number of Epoch = 15 - Accuracy:= 97.98253580729167\n",
            "\n",
            "Total time taken (in seconds): 1058.63\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 15\n",
        "\n",
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrhQbH38q5Kl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdfjh37sq5xP",
        "outputId": "2d552429-ff36-4141-d844-a0fbe1fcd493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0000\n",
            "96.57 %\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "# for inputs, outputs in test_ds:\n",
        "preds = mlp_on_gpu.forward(X_test)\n",
        "#b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "#List comprehension to map the lambda function across all records of y_true and y_pred\n",
        "y_true = np.array([maxposition(rec) for rec in y_test])\n",
        "y_pred = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(y_true == y_pred)/len(y_pred)\n",
        "print(val_acc*100,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D1lIS75vRbZ"
      },
      "source": [
        "5. Let's try L1 regularization with No. of epochs as 12 and learning rate as 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UgPKNzCvrI2"
      },
      "outputs": [],
      "source": [
        "# Initialize model using GPU\n",
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.06)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L1= (tf.reduce_sum(self.W1)+ tf.reduce_sum(self.W2)+tf.reduce_sum(self.W3)) \n",
        "      current_loss = self.loss(predicted, y_train) + 0.01 * L1 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBH3WVrVzeJQ",
        "outputId": "05a1ff60-5bc9-4685-d77d-2f1836639dc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.17444921875\n",
            "Number of Epoch = 1 - Accuracy:= 11.131665039062499\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.11697017415364583\n",
            "Number of Epoch = 2 - Accuracy:= 9.246722412109374\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.1165115234375\n",
            "Number of Epoch = 3 - Accuracy:= 9.358384195963541\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.11631156412760417\n",
            "Number of Epoch = 4 - Accuracy:= 9.385040283203125\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.116121875\n",
            "Number of Epoch = 5 - Accuracy:= 9.343365478515626\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.11597390950520833\n",
            "Number of Epoch = 6 - Accuracy:= 9.546696980794271\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.11585550944010417\n",
            "Number of Epoch = 7 - Accuracy:= 9.706705729166666\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.11573255208333333\n",
            "Number of Epoch = 8 - Accuracy:= 9.57003173828125\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.11559190266927083\n",
            "Number of Epoch = 9 - Accuracy:= 9.773372395833334\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.11544951171875\n",
            "Number of Epoch = 10 - Accuracy:= 10.141683959960938\n",
            "Number of Epoch = 11 - Categorical Cross-Entropy:= 0.115400390625\n",
            "Number of Epoch = 11 - Accuracy:= 10.235024007161458\n",
            "Number of Epoch = 12 - Categorical Cross-Entropy:= 0.11536094563802084\n",
            "Number of Epoch = 12 - Accuracy:= 10.351693725585937\n",
            "\n",
            "Total time taken (in seconds): 886.39\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 12\n",
        "\n",
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtBHz7rJziSz",
        "outputId": "b831b4cf-11c4-492d-da59-c8e186fde418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0002\n",
            "11.35 %\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "# for inputs, outputs in test_ds:\n",
        "preds = mlp_on_gpu.forward(X_test)\n",
        "#b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "#List comprehension to map the lambda function across all records of y_true and y_pred\n",
        "y_true = np.array([maxposition(rec) for rec in y_test])\n",
        "y_pred = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(y_true == y_pred)/len(y_pred)\n",
        "print(val_acc*100,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA9dTQhAPpJA"
      },
      "source": [
        "6. Taking L2 regularization, learning rate = 0.6 and epochs as 12\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6NBTQWy3sUf"
      },
      "outputs": [],
      "source": [
        "# Initialize model using GPU\n",
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.06)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 \n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBrlDF6T3x7V",
        "outputId": "96e195cb-3ee6-42ea-9e31-fb9a231db4ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.664136328125\n",
            "Number of Epoch = 1 - Accuracy:= 17.55501708984375\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.6180444661458333\n",
            "Number of Epoch = 2 - Accuracy:= 23.27833455403646\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.5873873046875\n",
            "Number of Epoch = 3 - Accuracy:= 27.088319905598958\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.5721716796875\n",
            "Number of Epoch = 4 - Accuracy:= 28.949945068359373\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.5018830403645833\n",
            "Number of Epoch = 5 - Accuracy:= 37.648356119791664\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.4923470703125\n",
            "Number of Epoch = 6 - Accuracy:= 38.839978027343754\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.42974576822916666\n",
            "Number of Epoch = 7 - Accuracy:= 46.21836344401042\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.10949156901041666\n",
            "Number of Epoch = 8 - Accuracy:= 79.539501953125\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.013144222005208333\n",
            "Number of Epoch = 9 - Accuracy:= 92.54397786458334\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.009225526936848959\n",
            "Number of Epoch = 10 - Accuracy:= 94.54391276041667\n",
            "Number of Epoch = 11 - Categorical Cross-Entropy:= 0.007399883015950521\n",
            "Number of Epoch = 11 - Accuracy:= 95.56553548177084\n",
            "Number of Epoch = 12 - Categorical Cross-Entropy:= 0.006199544270833334\n",
            "Number of Epoch = 12 - Accuracy:= 96.2738525390625\n",
            "\n",
            "Total time taken (in seconds): 828.44\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 12\n",
        "\n",
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxNp8f_aQG0c"
      },
      "source": [
        "7.Changing the optimizer to adam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "zzbQB4sBQPeM"
      },
      "outputs": [],
      "source": [
        "# Initialize model using GPU\n",
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.06)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 \n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "FfOrwK9_Qabx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64c10954-b772-4d8a-99a5-501bfdabdf5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.13606853841145833\n",
            "Number of Epoch = 1 - Accuracy:= 39.12495524088542\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.122042919921875\n",
            "Number of Epoch = 2 - Accuracy:= 24.961732991536458\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.12182252604166667\n",
            "Number of Epoch = 3 - Accuracy:= 30.61329549153646\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.13426326497395832\n",
            "Number of Epoch = 4 - Accuracy:= 29.980037434895834\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.13995704752604166\n",
            "Number of Epoch = 5 - Accuracy:= 30.783345540364582\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.1355442138671875\n",
            "Number of Epoch = 6 - Accuracy:= 31.97164916992187\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.1412935546875\n",
            "Number of Epoch = 7 - Accuracy:= 32.98833414713542\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.153947021484375\n",
            "Number of Epoch = 8 - Accuracy:= 27.666609700520834\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.14947438151041667\n",
            "Number of Epoch = 9 - Accuracy:= 21.41004842122396\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.18440011393229166\n",
            "Number of Epoch = 10 - Accuracy:= 18.81167195638021\n",
            "Number of Epoch = 11 - Categorical Cross-Entropy:= 0.18409970703125\n",
            "Number of Epoch = 11 - Accuracy:= 20.02001546223958\n",
            "Number of Epoch = 12 - Categorical Cross-Entropy:= 0.187221875\n",
            "Number of Epoch = 12 - Accuracy:= 18.874995930989584\n",
            "\n",
            "Total time taken (in seconds): 1486.79\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 12\n",
        "\n",
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajfISMFnXb19"
      },
      "source": [
        "8. Changing the batch size and changing the optimizer back to SGD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "l6EZ2QskVfjh"
      },
      "outputs": [],
      "source": [
        "# Initialize model using GPU\n",
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.06)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 \n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "COG3k7NcXu2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9084d949-d051-4c74-d3c8-ecc5bde4a450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 1.3462825520833333\n",
            "Number of Epoch = 1 - Accuracy:= 16.45828145345052\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 1.3095045572916666\n",
            "Number of Epoch = 2 - Accuracy:= 18.748126220703128\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.2975274739583333\n",
            "Number of Epoch = 3 - Accuracy:= 19.491448974609373\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.2945876302083332\n",
            "Number of Epoch = 4 - Accuracy:= 19.67147216796875\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 1.30822421875\n",
            "Number of Epoch = 5 - Accuracy:= 18.796429443359376\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.3752591796875\n",
            "Number of Epoch = 6 - Accuracy:= 69.2760009765625\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.018576680501302085\n",
            "Number of Epoch = 7 - Accuracy:= 94.44149576822916\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.012700848388671875\n",
            "Number of Epoch = 8 - Accuracy:= 96.12504069010417\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.009764949544270833\n",
            "Number of Epoch = 9 - Accuracy:= 97.0487548828125\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.008404271952311198\n",
            "Number of Epoch = 10 - Accuracy:= 97.4587646484375\n",
            "Number of Epoch = 11 - Categorical Cross-Entropy:= 0.007543875630696615\n",
            "Number of Epoch = 11 - Accuracy:= 97.7355224609375\n",
            "Number of Epoch = 12 - Categorical Cross-Entropy:= 0.006955970255533854\n",
            "Number of Epoch = 12 - Accuracy:= 97.99730631510417\n",
            "\n",
            "Total time taken (in seconds): 1613.44\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 12\n",
        "\n",
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(10)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 10 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhu9Zfh-sXj9"
      },
      "source": [
        "Changing the batch size to 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "Ed6VF_SbivfA"
      },
      "outputs": [],
      "source": [
        "# Initialize model using GPU\n",
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.06)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 \n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "PxQMY4ICi3Hr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "839ffee3-c42c-4d05-d214-166ba5c83f65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.4811564453125\n",
            "Number of Epoch = 1 - Accuracy:= 10.441703796386719\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.4811638671875\n",
            "Number of Epoch = 2 - Accuracy:= 10.44170913696289\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.48116412760416666\n",
            "Number of Epoch = 3 - Accuracy:= 10.441705322265625\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.48116373697916665\n",
            "Number of Epoch = 4 - Accuracy:= 10.441706848144532\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.4811639973958333\n",
            "Number of Epoch = 5 - Accuracy:= 10.44171371459961\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.4811639973958333\n",
            "Number of Epoch = 6 - Accuracy:= 10.441694641113282\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.48116393229166665\n",
            "Number of Epoch = 7 - Accuracy:= 10.44170150756836\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.48116396484375\n",
            "Number of Epoch = 8 - Accuracy:= 10.441700744628907\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.4452951171875\n",
            "Number of Epoch = 9 - Accuracy:= 16.806642150878908\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.047677742513020836\n",
            "Number of Epoch = 10 - Accuracy:= 85.21921997070312\n",
            "Number of Epoch = 11 - Categorical Cross-Entropy:= 0.009956605021158854\n",
            "Number of Epoch = 11 - Accuracy:= 92.00586547851563\n",
            "Number of Epoch = 12 - Categorical Cross-Entropy:= 0.006806508382161458\n",
            "Number of Epoch = 12 - Accuracy:= 93.990771484375\n",
            "\n",
            "Total time taken (in seconds): 700.17\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 12\n",
        "\n",
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(30)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 30 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "k7XVbcnhuFv1"
      },
      "outputs": [],
      "source": [
        "# Initialize model using GPU\n",
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.06)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 \n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2 \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "KLvewF9RuMfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf1dd2bf-1048-43a9-f852-a70b62b0f190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.40784563802083335\n",
            "Number of Epoch = 1 - Accuracy:= 24.006585693359376\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.29243151041666665\n",
            "Number of Epoch = 2 - Accuracy:= 45.49493408203125\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.28340244140625\n",
            "Number of Epoch = 3 - Accuracy:= 47.168328857421876\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.2780332356770833\n",
            "Number of Epoch = 4 - Accuracy:= 48.18335876464844\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.2749978515625\n",
            "Number of Epoch = 5 - Accuracy:= 48.7417236328125\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.27373271484375\n",
            "Number of Epoch = 6 - Accuracy:= 48.974954223632814\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.27136612955729167\n",
            "Number of Epoch = 7 - Accuracy:= 49.40827331542968\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.23519226888020833\n",
            "Number of Epoch = 8 - Accuracy:= 55.95648193359375\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.22406306966145834\n",
            "Number of Epoch = 9 - Accuracy:= 58.064874267578126\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.20334490559895832\n",
            "Number of Epoch = 10 - Accuracy:= 61.48856201171875\n",
            "Number of Epoch = 11 - Categorical Cross-Entropy:= 0.023205631510416668\n",
            "Number of Epoch = 11 - Accuracy:= 90.5909423828125\n",
            "Number of Epoch = 12 - Categorical Cross-Entropy:= 0.008326595052083333\n",
            "Number of Epoch = 12 - Accuracy:= 93.59742431640625\n",
            "\n",
            "Total time taken (in seconds): 655.89\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 12\n",
        "\n",
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5510)).batch(30)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 30 / X_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bXBhwWh2jcm"
      },
      "outputs": [],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "# for inputs, outputs in test_ds:\n",
        "preds = mlp_on_gpu.forward(X_test)\n",
        "#b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "#List comprehension to map the lambda function across all records of y_true and y_pred\n",
        "y_true = np.array([maxposition(rec) for rec in y_test])\n",
        "y_pred = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(y_true == y_pred)/len(y_pred)\n",
        "print(val_acc*100,\"%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Mnist",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}